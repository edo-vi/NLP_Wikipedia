{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data, classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data will be a set of articles downloaded from Wikipedia that we need to classify as concerning 'Medical' topics, or 'Non medical'; in particular, we need to classify documents in two classes: those with a 'Medical' tags, and those without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataset that can be used for the classification task we need to download articles from Wikipedia API. Wikipedia already groups articles under different categories, so for this binary classification task we can just use their functions to select a category and the download articles belonging to that category.\n",
    "\n",
    "Wikipedia categories are highly hierarchical, so, for e.g., the category 'Medicine' contains few articles and many subcategories, which are themselves articles. To 'fix' the search I did some exploratory analysis of the available categories to determine which had enouigh articles to build a decently-sized dataset. \n",
    "\n",
    "For the medical catogories, I went with the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_categories = [\n",
    "    \"Category:Alternative medicine stubs\",\n",
    "    \"Category:Evidence-based medicine\",\n",
    "    \"Category:Veterinary medicine stubs\",\n",
    "    \"Category:Vaccination\",\n",
    "    \"Category:2018 disease outbreaks\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the non-medical categories, I went with scientific and non-scientific categories: . I did explicity add scientific/related categories to make the classification task more interesting: i expect an article belonging to the 'Literature' to be classified correctly quite easily; an article belonging to 'Geology' or 'AI', not so much (due to possible overlapping topics).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_medical_categories = [\n",
    "    \"Category:Dark ages\",\n",
    "    \"Category:Historiography of China\",\n",
    "    \"Category:Sports controversies\",\n",
    "    \"Category:Philosophy of artificial intelligence\",\n",
    "    \"Category:Geology\",\n",
    "    \"Category:Space\",\n",
    "    \"Category:Literature\",\n",
    "    \"Category:Music videos\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use these categories to download the articles ids using the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_ids(url, categories):\n",
    "    returned_ids = []\n",
    "\n",
    "    for c in categories:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"cmtitle\": c,\n",
    "            \"cmlimit\": \"500\",\n",
    "            # \"cmtype\": \"subcat\",\n",
    "            \"list\": \"categorymembers\",\n",
    "            \"format\": \"json\",\n",
    "        }\n",
    "\n",
    "        req = requests.get(url=url, params=params)\n",
    "        pages = req.json()[\"query\"][\"categorymembers\"]\n",
    "\n",
    "        page_ids = [page[\"pageid\"] for page in pages]\n",
    "\n",
    "        for id in page_ids:\n",
    "            new_params = {\n",
    "                \"format\": \"json\",\n",
    "                \"action\": \"query\",\n",
    "                \"prop\": \"extracts\",\n",
    "                \"exintro\": True,\n",
    "                \"explaintext\": True,\n",
    "                \"redirects\": 1,\n",
    "                \"pageids\": id,\n",
    "            }\n",
    "            req = requests.get(url, new_params)\n",
    "            try:\n",
    "                title = req.json()[\"query\"][\"pages\"][str(id)][\"title\"]\n",
    "                # print(title)\n",
    "                if (\n",
    "                    title.startswith(\"Category\")\n",
    "                    or title.startswith(\"Template\")\n",
    "                    or title.startswith(\"Portal\")\n",
    "                ):\n",
    "                    continue\n",
    "                else:\n",
    "                    returned_ids.append(id)\n",
    "            except:\n",
    "                print(f\"||Failed at id {id}||\")\n",
    "\n",
    "    return returned_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/w/api.php\"\n",
    "# We download them like this\n",
    "__M_IDS__ = get_ids(url, medical_categories)\n",
    "__NON_M_IDS__ = get_ids(url, non_medical_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ids and dataset sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical ids (clipped): [46303046, 5835531, 3887850, 52103535, 33076141, 5335383, 40440492, 4031180, 66655182, 55554061, 951614, 66012376, 10420896, 3740763, 3296243, 6868776, 38474884, 300772, 10616040, 36732930] ...\n",
      "\n",
      "Non medical ids (clipped): [7410249, 18400571, 90138, 5571005, 53290497, 5832437, 48110, 1840762, 2958015, 8099572, 18472072, 4513331, 25508360, 3054853, 68092158, 34043, 440393, 36082813, 4175228, 13666328] ... \n",
      "\n",
      "Size of the medical dataset: 457\n",
      "Size of the non-medical dataset: 399\n",
      "Total dataset size: 856\n"
     ]
    }
   ],
   "source": [
    "# For presentation purposes, we show the one that we have already used, without needing to compute them another time\n",
    "from ids import __M_IDS__, __NON_M_IDS__\n",
    "\n",
    "print(f\"Medical ids (clipped): {__M_IDS__[:20]} ...\\n\")\n",
    "print(f\"Non medical ids (clipped): {__NON_M_IDS__[:20]} ... \\n\")\n",
    "\n",
    "print(f\"Size of the medical dataset: {len(__M_IDS__)}\")\n",
    "print(f\"Size of the non-medical dataset: {len(__NON_M_IDS__)}\")\n",
    "print(f\"Total dataset size: {len(__M_IDS__) + len(__NON_M_IDS__)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now two sets of ids: medical ids and non-medical ids. We will use them to download articles and build our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"ids\": ids used of downloaded documents; \n",
    "# \"kind\": medical or non-medical, used to separate the two types of documents into two folders for ease of use\n",
    "def download_documents(ids, kind):\n",
    "    for id in ids:\n",
    "        new_params = {\n",
    "            \"format\": \"json\",\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"revisions\",\n",
    "            \"rvslots\": \"*\",\n",
    "            \"rvprop\": \"content\",\n",
    "            \"redirects\": 1,\n",
    "            \"pageids\": id,\n",
    "        }\n",
    "        req = requests.get(url, new_params)\n",
    "        try:\n",
    "            title = req.json()[\"query\"][\"pages\"][str(id)][\"revisions\"][0][\"slots\"][\n",
    "                \"main\"\n",
    "            ][\"*\"]\n",
    "            with open(f\"./documents/{kind}/{id}.txt\", \"w\") as f:\n",
    "                f.write(title)\n",
    "        except:\n",
    "            print(f\"||Failed at id {id}||\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download all the documents and store them in 'documents/medicine' and 'documents/non_medicine'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Medical**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{Short description|University in Tianjin, China}}\n",
    "'''Tianjin University of Traditional Chinese Medicine''' (天津中医药大学 in [[Chinese language|Chinese]]) is  a university in [[Tianjin]], [[China]], under the municipal government. Specialized in traditional Chinese Medicine, it is selected by the Chinese state [[Double First Class University Plan|Double First-Class University]], included in the national Double First Class University Plan.<ref name=\"Chinese Department of Education\">{{Cite web |url=http://www.moe.gov.cn/srcsite/A22/moe_843/201709/t20170921_314942.html |title=教育部 财政部 国家发展改革委 关于公布世界一流大学和一流学科建设高校及建设 学科名单的通知 (Notice from the Ministry of Education and other national governmental departments announcing the list of double first class universities and disciplines)}}</ref>\n",
    "\n",
    "== See also ==\n",
    "[[Japan Campus of Foreign Universities]]\n",
    "\n",
    "== References ==\n",
    "{{Reflist}}\n",
    "\n",
    "{{-}}\n",
    "{{Universities and colleges in Tianjin}}\n",
    "\n",
    "{{coord missing|Tianjin}}\n",
    "\n",
    "{{authority control}}\n",
    "\n",
    "{{DEFAULTSORT:Tianjin University of Traditional Chinese Medicine}}\n",
    "[[Category:Universities and colleges in Tianjin]]\n",
    "[[Category:Traditional Chinese medical schools in China]]\n",
    "[[Category:Medical and health organizations based in China]]\n",
    "\n",
    "\n",
    "{{China-university-stub}}\n",
    "{{Alt-med-stub}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-medical**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{Short description|Abbreviation of 1,000,000 years}}{{redirect|Million years ago|the [[Adele]] song|Million Years Ago (song)|1,000,000 BC|one million (disambiguation)}}\n",
    "{{about|\"million years\" (Myr)|the [[Taake]] song|Noregs vaapen|other uses}}\n",
    "\n",
    "'''Myr''' is an abbreviation for '''million years''', a [[unit of time]] equal to {{val|fmt=commas|1000000|u=years}} (i.e. {{val|1|e=6}} years), or 31.556926 [[Terasecond and longer#Teraseconds|teraseconds]].\n",
    "It is equivalent to one ''[[megaannum]]'' (symbol Ma), based on the [[metric prefix]] [[mega-]].\n",
    "\n",
    "==Usage==\n",
    "Myr (million years) is in common use in fields such as [[Earth science]] and [[cosmology]]. Myr is also used with '''Mya''' or '''Ma''' (million years ago). Together they make a reference system, one to a quantity, the other to a particular place in a [[calendar era|year numbering system]] that is ''time before the present''.\n",
    "\n",
    "Myr is deprecated in [[geology]], but in [[astronomy]] ''Myr'' is standard. Where \"myr\" ''is'' seen in geology it is usually \"Myr\" (a unit of mega-years). In astronomy it is usually \"Myr\" (Million years).\n",
    "\n",
    "== Debate ==\n",
    "In geology a debate remains open concerning the use of ''Myr'' (duration) plus ''Ma'' (million years ago) versus using only the term ''Ma''.<ref>{{cite web|last=Mozley|first=Peter|title=Discussion of GSA Time Unit Conventions|url=https://www.geosociety.org/TimeUnits/|work=web page|publisher=[[Geological Society of America]]|archive-url=https://web.archive.org/web/20160303232640/https://www.geosociety.org/TimeUnits/|archive-date=2016-03-03}}</ref><ref name=\"Biever-war\">{{cite journal |first=Celeste |last=Biever |title=Push to define year sparks time war |journal=[[New Scientist]] |volume=210 |issue=2810 |pages=10 |url=https://www.newscientist.com/article/dn20423-push-to-define-year-sparks-time-war.html |date=April 27, 2011 |access-date=April 28, 2011|bibcode=2011NewSc.210R..10B |doi=10.1016/S0262-4079(11)60955-X }}</ref> In either case the term ''[[Year#SI prefix multipliers|Ma]]'' is used in geology literature conforming to [[ISO 31-1]] (now [[ISO 80000-3]]) and NIST 811 recommended practices. Traditional style geology literature is written {{Quote|The Cretaceous started 145 Ma and ended 66 Ma, lasting for 79 Myr.}}\n",
    "The \"ago\" is implied, so that any such year number \"X Ma\" between 66 and 145 is \"Cretaceous\", for good reason. But the counter argument is that having ''myr'' for a duration and ''Mya'' for an age mixes unit systems, and tempts capitalization errors: \"million\" need not be capitalized, but \"mega\" must be; \"ma\" would technically imply a ''milliyear'' (a thousandth of a year, or 8 hours). On this side of the debate, one avoids ''myr'' and simply adds ''ago'' explicitly (or adds ''[[Before Present|BP]]''), as in {{Quote|The Cretaceous started 145 Ma ago and ended 66 Ma ago, lasting for 79 Ma.}}\n",
    "In this case, \"79 Ma\" means only a quantity of 79 million years, without the meaning of \"79 million years ago\".\n",
    "\n",
    "== See also ==\n",
    "* [[Billion years|Byr]]\n",
    "* [[Kyr]]\n",
    "* [[Year#SI prefix multipliers|Megaannum]] (Ma)\n",
    "* [[Year#Abbreviations yr and ya|Symbols y and yr]]\n",
    "\n",
    "==References==\n",
    "<references/>\n",
    "\n",
    "{{Portal bar|Earth science|Mathematics|Astronomy|Stars}}\n",
    "\n",
    "[[Category:Units of time]]\n",
    "[[Category:Units of measurement in astronomy]]\n",
    "[[Category:Geology]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, they are very raw and have lots of useless, for the purpose of classification, symbols and words that are used by Wikipedia to render and refer to the article. We need to clean these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Given a folder containing the desired documents, clean them and save them in './documents'.\n",
    "def clean_documents(folder):\n",
    "    path = f\"./documents/{folder}\"\n",
    "    os.chdir(path)\n",
    "    for file in os.listdir():\n",
    "        # Check whether file is in text format or not\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{path}/{file}\"\n",
    "\n",
    "            new_lines = []\n",
    "            with open(file, \"r\") as f:\n",
    "                # All lines contained in the file\n",
    "                lines = f.readlines()\n",
    "                for l in lines:\n",
    "                    # Clean line and add to new_lines\n",
    "                    new_lines.append(clean_string(l))\n",
    "            f_path = file.split(\".\")[0]\n",
    "            new_path = f\"../{f_path}_c.txt\"\n",
    "            # Save to \"./documents/{file_path}_c.txt\"\n",
    "            with open(new_path, \"w\") as fw:\n",
    "                for nl in new_lines:\n",
    "                    fw.write(nl)\n",
    "\n",
    "# Clean a string. These are heuristic-based rules that work on Wikipedia articles\n",
    "def clean_string(string):\n",
    "    string = re.sub(\"<ref.*?</ref>\", \"\", string)  # removes refs\n",
    "    string = re.sub(\"<ref.*?/>\", \"\", string)  # idem\n",
    "    string = re.sub(\"{.*?}\", \"\", string)  # removes \"{...}\"\n",
    "    string = re.sub(\n",
    "        \"\\|.*\\n?\", \"\", string\n",
    "    )  # removes lines starting with \"|\"\" and continuing until the end\n",
    "    string = re.sub(\"(Category).*\\n?\", \"\", string)\n",
    "    string = re.sub(\"(thumb\\|.*?\\|)\", \"\", string)  # removes \"thumb|...|\"\n",
    "    string = re.sub(\n",
    "        \"(thumb)\", \"\", string\n",
    "    )  # removes \"thumb\" (cannot easily distinguish all cases)\n",
    "    string = re.sub(\n",
    "        \"\\[\\[.*?\\|\", \"\", string\n",
    "    )  # removes links such as [[dieting|diet]], but only the first part (up until \"|\"), which is the link.\n",
    "    string = re.sub(\n",
    "        \"[\\[,\\],{,},',\\\\',\\,\\.,#,=,*\\|`-]\", \"\", string\n",
    "    )  # removes all remaining bad characters: left out [], {}, #, =, |, ', `, -, *\n",
    "    string = re.sub(\"\\\\n\", \"\\n\", string)  # removes newlines\n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a541b3ac8cb6bde8bf2f40370b4b304d430b09de9e456ad867feddab99cbf32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
