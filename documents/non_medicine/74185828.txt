{{Short description|none}}
{{Orphan|date=July 2023}}

'''Artificial intelligence and moral enhancement''' involves the application of [[artificial intelligence]] to the enhancement of [[moral reasoning]] and the acceleration of [[moral progress]].

==Artificial moral reasoning==
{{see also|Machine_ethics|l1=Machine ethics}}

With respect to [[moral reasoning]], some consider humans to be suboptimal information processors, moral judges, and moral agents.<ref name="giubilini2018artificial">{{cite journal |title=The Artificial Moral Advisor: The "Ideal Observer" Meets Artificial Intelligence |journal=Philosophy & Technology |year=2018 |last=Giubilini |first=Alberto |last2=Savulescu |first2=Julian |volume=31 |pages=169–188 |url=https://link.springer.com/article/10.1007/s13347-017-0285-z |accessdate=2023-07-01 }}</ref> Due to stress or time constraints, people often fail to consider all the relevant factors and information necessary to make well-reasoned [[Moral reasoning|moral judgments]], people lack consistency, and they are prone to [[Cognitive bias|biases]].

With the rise of [[artificial intelligence]], [[Moral agency#Artificial moral agents|artificial moral agents]] can perform and enhance [[moral reasoning]], overcoming human limitations.

==Ideal observer theory==
{{see also|Ideal_observer_theory|l1=Ideal observer theory}}

The classical [[ideal observer theory]] is a [[metaethics|metaethical]] theory about the meaning of moral statements. It holds that a moral statement is any statement to which an "ideal observer" would react or respond in a certain way. An ideal observer is defined as being: (1) omniscient with respect to non-ethical facts, (2) omnipercipient, (3) disinterested, (4) dispassionate, (5) consistent, and (6) normal in all other respects.

[[Adam Smith]] and [[David Hume]] espoused versions of the [[ideal observer theory]] and [[Roderick Firth]] provided a more sophisticated and modern version.<ref>{{cite journal |title=Ethical Absolutism and the Ideal Observer |first=Roderick |last=Firth |journal=Philosophy and Phenomenological Research |volume=12 |issue=3 |date=March 1952 |pages=317–345 |jstor=2103988}}</ref> An analogous idea in law is the [[reasonable person]] criterion.

Today, [[artificial intelligence]] systems are capable of providing or assisting in moral decisions, stating what we ought to morally do if we want to comply with certain moral principles.<ref name="giubilini2018artificial" /> [[Artificial intelligence]] systems can gather information from environments, process it utilizing operational criteria, e.g., moral criteria such as [[values]], [[goal]]s, and [[principles]], and advise users on morally best courses of action.<ref name="savulescu2015moral">{{cite book |chapter=Moral Enhancement and Artificial Intelligence: Moral AI? |title=Beyond Artificial Intelligence: The Disappearing Human-machine Divide |year=2015 |last=Savulescu |first=Julian |last2=Maslen |first2=Hannah |editor-last1=Romportl |editor-first1=Jan |editor-last2=Zackova |editor-first2=Eva |editor-last3=Kelemen |editor-first3=Jozef |pages=79–95 |url=https://link.springer.com/chapter/10.1007/978-3-319-09668-1_6}}</ref> These systems can enable humans to make (nearly) optimal moral choices that we do not or cannot usually perform because of lack of necessary mental resources or time constraints.

Artificial moral advisors can be compared and contrasted with [[ideal observer theory|ideal observers]].<ref name="giubilini2018artificial" /> [[ideal observer theory|Ideal observers]] have to be omniscient and omnipercipient about non-ethical facts, while artificial moral advisors would just need to know those morally relevant facts which pertain to a decision.

Users can provide varying configurations and settings to instruct these systems, and this allows these systems to be [[relativist]]. Relativist artificial moral advisors would equip humans to be better moral judges and would respect their autonomy as both moral judges and moral agents.<ref name="giubilini2018artificial" /> For these reasons, and because artificial moral advisors would be disinterested, dispassionate, consistent, relational, dispositional, empirical, and objectivist, [[relativist]] artificial moral advisors could be preferable to [[Universality (philosophy)|absolutist]] ideal observers.<ref name="giubilini2018artificial" />

==Exhaustive versus auxiliary enhancement==
Exhaustive enhancement involves scenarios where human moral decision-making is supplanted, left entirely to machines. Some proponents consider machines as being morally superior to humans and that just doing as the machines say would constitute moral improvement.<ref name="volkman2023ai">{{cite journal |title=AI Moral Enhancement: Upgrading the Socio-Technical System of Moral Engagement |journal=Science and Engineering Ethics |year=2023 |last=Volkman |first=Richard |last2=Gabriels |first2=Katleen |volume=29 |issue=2 |url=https://link.springer.com/article/10.1007/s11948-023-00428-2 |accessdate=2023-07-01 }}</ref>

Opponents of exhaustive enhancement list five main concerns:<ref name="lara2020artificial">{{cite journal |title=Artificial Intelligence as a Socratic Assistant for Moral Enhancement |journal=Neuroethics |year=2020 |last=Lara |first=Francisco |last2=Deckers |first2=Jan |volume=13 |issue=3 |pages=275–287 |url=https://link.springer.com/article/10.1007/s12152-019-09401-y |accessdate=2023-07-01 }}</ref> (1) the existence of [[Pluralism_(political_philosophy)|pluralism]] may complicate finding [[Consensus decision-making|consensus]]es on which to build, configure, train, or inform systems, (2) even if such consensuses could be achieved, people might still fail to construct good systems due to human or nonhuman limitations, (3) resultant systems might not be able to make autonomous moral decisions, (4) [[moral progress]] might be hindered, (5) it would mean the death of [[morality]].

Dependence on artificial intelligence systems to perform moral reasoning would not only neglect the cultivation of moral excellence but actively undermine it, exposing people to risks of disengagement, of atrophy of human faculties, and of moral manipulation at the hands of the systems or their creators.<ref name="volkman2023ai"/>

Auxiliary enhancement addresses these concerns and involves scenarios where machines augment or supplement human decision-making. Artificial intelligence assistants would be tools to help people to clarify and keep track of their moral commitments and contexts while providing accompanying [[explanation]]s, [[argument]]s, and [[Justification (epistemology)|justifications]] for conclusions. The ultimate decision-making, however, would rest with the human users.<ref name="volkman2023ai"/>

Some proponents of auxiliary enhancement also support [[Educational technology|educational technologies]] with respect to [[morality]], technologies which teach [[moral reasoning]], e.g., assistants which utilize the [[Socratic method]].<ref name="lara2020artificial" /> It may be the case that a “right” or “best” answer to a moral question is a “best” dialogue which provides value for users.

==Pluralism==
[[Moral agency#Artificial moral agents|Artificial moral agents]] could be made to be configurable so as to be able to match the moral commitments of their users. This would preserve the existing [[Pluralism_(political_philosophy)|pluralism]] in societies.<ref name="savulescu2015moral" />

Beyond matching their users’ moral commitments, [[Moral agency#Artificial moral agents|artificial moral agents]] could emulate historical or contemporary [[Philosophy|philosophers]] and could adopt and utilize [[Point of view (philosophy)|points of view]], [[School of thought|schools of thought]], or [[Religion|wisdom traditions]].<ref name="volkman2023ai"/> Responses produced by teams composed of [[Multi-agent systems|multiple]] [[Moral agency#Artificial moral agents|artificial moral agents]] could be a result of [[debate]] or other processes for combining their individual outputs.

==See also==
* [[AI alignment]]
* [[Artificial intelligence]]
* [[Automated decision-making]]
* [[Decision support system]]
* [[Intelligent tutoring system]]
* [[Legal informatics]]
* [[Machine ethics]]
* [[Moral reasoning]]
* [[Multi-agent systems]]
* [[Project Debater]]
* [[Superintelligence]]

==References==
{{reflist}}

[[Category:Bioethics]]
[[Category:Philosophy of artificial intelligence|Moral enhancement]]
[[Category:Morality]]