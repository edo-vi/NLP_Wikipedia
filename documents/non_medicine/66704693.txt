{{short description|Risks of astronomical suffering}}
[[File:X-risk-chart-en-01a.svg|thumb|right|250px|Suffering risks have a similar scope and severity as existential risks.<ref>{{cite interview|url=https://futureoflife.org/2018/06/14/podcast-astronomical-future-suffering-and-superintelligence-with-kaj-sotala/|title=AI Alignment Podcast: Astronomical Future Suffering and Superintelligence with Kaj Sotala|last=Sotala|first=Kaj|interviewer=Lucas Perry|date=June 14, 2018|publisher=Future of Life Institute|access-date=June 4, 2022}}</ref>]]

'''Suffering risks''', or '''s-risks''', are risks involving an astronomical amount of [[suffering]], much more than all of the suffering having occurred on Earth.<ref>{{Cite web |last=Daniel |first=Max |date=2017-06-20 |title=S-risks: Why they are the worst existential risks, and how to prevent them (EAG Boston 2017) |url=https://longtermrisk.org/s-risks-talk-eag-boston-2017/ |access-date=2023-09-14 |website=Center on Long-Term Risk |language=en-US}}</ref><ref>{{Cite web |last=Hilton |first=Benjamin |date=September 2022 |title='S-risks' |url=https://80000hours.org/problem-profiles/s-risks/ |access-date=2023-09-14 |website=80,000 Hours |language=en-US}}</ref> They are sometimes categorized as a subclass of [[Global catastrophic risk|existential risks]].<ref>{{Cite web |last=Baumann |first=Tobias |date=2017 |title=S-risk FAQ |url=https://centerforreducingsuffering.org/research/faq/ |access-date=2023-09-14 |website=Center for Reducing Suffering |language=en-US}}</ref>

Sources of possible s-risks include embodied [[artificial intelligence]]<ref name=":2">{{Cite journal|last1=Umbrello|first1=Steven|last2=Sorgner|first2=Stefan Lorenz|date=June 2019|title=Nonconscious Cognitive Suffering: Considering Suffering Risks of Embodied Artificial Intelligence|journal=Philosophies|language=en|volume=4|issue=2|pages=24|doi=10.3390/philosophies4020024|doi-access=free}}</ref> and [[superintelligence]],<ref name=":0">{{Cite journal|last1=Sotala|first1=Kaj|last2=Gloor|first2=Lukas|date=2017-12-27|title=Superintelligence As a Cause or Cure For Risks of Astronomical Suffering|url=http://www.informatica.si/index.php/informatica/article/view/1877|journal=Informatica|language=en|volume=41|issue=4|issn=1854-3871}}</ref> as well as [[space colonization]], which could potentially lead to "constant and catastrophic wars"<ref>{{Cite journal|last=Torres|first=Phil|date=2018-06-01|title=Space colonization and suffering risks: Reassessing the "maxipok rule"|url=https://www.sciencedirect.com/science/article/pii/S0016328717304056|journal=Futures|language=en|volume=100|pages=74–85|doi=10.1016/j.futures.2018.04.008|s2cid=149794325|issn=0016-3287}}</ref> and an immense increase in [[wild animal suffering]] by introducing wild animals, who "generally lead short, miserable lives full of sometimes the most brutal suffering", to other planets, either intentionally or inadvertently.<ref name=":1">{{Cite journal |last=Kovic |first=Marko |date=2021-02-01 |title=Risks of space colonization |url=https://www.sciencedirect.com/science/article/pii/S0016328720301270 |journal=Futures |language=en |volume=126 |pages=102638 |doi=10.1016/j.futures.2020.102638 |issn=0016-3287 |s2cid=230597480}}</ref> 

Steven Umbrello, an [[AI ethics]] researcher, has warned that [[biological computing]] may make system design more prone to s-risks.<ref name=":2" />

== References ==
<references />

== Further reading ==
* {{Cite book |last=Baumann |first=Tobias |url=https://centerforreducingsuffering.org/books/avoiding-the-worst-how-to-prevent-a-moral-catastrophe-by-tobias-baumann/ |title=Avoiding the Worst: How to Prevent a Moral Catastrophe |publisher=Independently published |year=2022 |isbn=979-8359800037 |language=en}}
* {{Cite journal|last=Metzinger|first=Thomas|author-link=Thomas Metzinger|date=2021-02-19|title=Artificial Suffering: An Argument for a Global Moratorium on Synthetic Phenomenology|journal=Journal of Artificial Intelligence and Consciousness|volume=08|language=en|pages=43–66|doi=10.1142/S270507852150003X|issn=2705-0785|doi-access=free}}
* {{cite news|last1=Minardi|first1=Di|title=The grim fate that could be 'worse than extinction'|url=https://www.bbc.com/future/article/20201014-totalitarian-world-in-chains-artificial-intelligence|access-date=2021-02-11|work=[[BBC Future]]|date=2020-10-15|language=en}}
* {{Cite web|last=Baumann|first=Tobias|date=2017|title=S-risks: An introduction|url=https://centerforreducingsuffering.org/intro/|archive-url=|archive-date=|access-date=2021-02-10|website=Center for Reducing Suffering|language=en-US}}
* {{Cite news|last1=Althaus|first1=David|last2=Gloor|first2=Lukas|date=2016-09-14|title=Reducing Risks of Astronomical Suffering: A Neglected Priority|url=https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/|archive-url=|archive-date=|access-date=2021-02-10|website=Center on Long-Term Risk|language=en-US}}

== See also ==
* [[AI control problem]]
* [[Ethics of artificial intelligence]]
* [[Ethics of terraforming]]
* [[Existential risk from artificial general intelligence]]
* [[Global catastrophic risk]]
* [[Suffering-focused ethics]]
*[[Wild animal suffering]]

{{Doomsday}}
{{Effective altruism}}
{{Existential risk from artificial intelligence}}

[[Category:Future problems]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Risk]]
[[Category:Space colonization]]
[[Category:Suffering]]
[[Category:Technology hazards]]
[[Category:Terraforming]]


{{futures-studies-stub}}
{{Astrobiology-stub}}