{{Short description|Overview of the philosophy of artificial intelligence}}
{{See also|Ethics of artificial intelligence}}
{{Artificial intelligence}}

The '''philosophy of artificial intelligence''' is a branch of the [[philosophy of mind]] and the [[philosophy of computer science]]<ref>{{cite web |title=Philosophy of Computer Science |url=https://www.oxfordbibliographies.com/display/document/obo-9780195396577/obo-9780195396577-0224.xml#obo-9780195396577-0224-div1-0019 |website=obo |language=en}}</ref> that explores [[artificial intelligence]] and its implications for knowledge and understanding of [[intelligence]], [[ethics]], [[consciousness]], [[epistemology]], and [[free will]].<ref>{{Cite web|url=http://jmc.stanford.edu/articles/aiphil2.html|title=The Philosophy of AI and the AI of Philosophy|last=McCarthy|first=John|website=jmc.stanford.edu|access-date=2018-09-18|archive-url=https://web.archive.org/web/20181023181725/http://jmc.stanford.edu/articles/aiphil2.html|archive-date=2018-10-23|url-status=dead}}</ref><ref name=":0">{{Cite journal |last=Müller |first=Vincent C. |date=2023-07-24 |title=Philosophy of AI: A structured overview |url=https://philpapers.org/rec/MLLPOA |journal=Nathalie A. Smuha (Ed.), Cambridge Handbook on the Law, Ethics and Policy of Artificial Intelligence}}</ref> Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see [[artificial life]]) so the discipline is of considerable interest to philosophers.<ref name=sep>{{Citation|last1=Bringsjord|first1=Selmer|title=Artificial Intelligence|date=2018|url=https://plato.stanford.edu/archives/fall2018/entries/artificial-intelligence/|encyclopedia=The Stanford Encyclopedia of Philosophy|editor-last=Zalta|editor-first=Edward N.|edition=Fall 2018|publisher=Metaphysics Research Lab, Stanford University|access-date=2018-09-18|last2=Govindarajulu|first2=Naveen Sundar|archive-url=https://web.archive.org/web/20191109002442/https://plato.stanford.edu/archives/fall2018/entries/artificial-intelligence/|archive-date=2019-11-09|url-status=dead}}</ref> These factors contributed to the emergence of the '''philosophy of artificial intelligence'''. 

The philosophy of artificial intelligence attempts to answer such questions as follows:<ref>{{Harvnb|Russell|Norvig|2003|p=947}} define the philosophy of AI as consisting of the first two questions, and the additional question of the [[ethics of artificial intelligence]]. {{Harvnb|Fearn|2007|p=55}} writes "In the current literature, philosophy has two chief roles: to determine whether or not such machines would be conscious, and, second, to predict whether or not such machines are possible." The last question bears on the first two.</ref>

* Can a machine act intelligently? Can it solve ''any'' problem that a person would solve by thinking?
* Are human intelligence and machine intelligence the same?  Is the [[human brain]] essentially a computer?
* Can a machine have a [[philosophy of mind|mind]], mental states, and [[consciousness]] in the same sense that a human being can? Can it ''feel how things are''?
Questions like these reflect the divergent interests of [[artificial intelligence|AI researchers]], [[cognitive science|cognitive scientists]] and [[philosophy|philosopher]]s respectively. The scientific answers to these questions depend on the definition of "intelligence" and "consciousness" and exactly which "machines" are under discussion.

Important [[proposition]]s in the philosophy of AI include some of the following:

*[[Turing test|Turing's "polite convention"]]: If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.<ref name=T>This is a paraphrase of the essential point of the [[Turing test]]. {{Harvnb|Turing|1950}}, {{Harvnb|Haugeland|1985|pp=6–9}}, {{Harvnb|Crevier|1993|p=24}}, {{Harvnb|Russell|Norvig|2003|pp=2–3 and 948}}</ref>
* The [[Dartmouth Workshop#Planning the Summer Research Project: The Proposal|Dartmouth proposal]]: "Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."<ref name=MMRS>{{Harvnb|McCarthy|Minsky|Rochester|Shannon|1955}}. This assertion was printed in the program for the [[Dartmouth Conferences|Dartmouth Conference]] of 1956, widely considered the "birth of AI."also {{Harvnb|Crevier|1993|p=28}}</ref>
* [[Allen Newell]] and [[Herbert A. Simon]]'s [[physical symbol system]] hypothesis: "A physical symbol system has the necessary and sufficient means of general intelligent action."<ref name=NS>{{Harvnb|Newell|Simon|1976}} and {{Harvnb|Russell|Norvig|2003|p=18}}</ref>
* [[John Searle]]'s [[strong AI hypothesis]]: "The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."<ref name=SWAI>This version is from {{Harvtxt|Searle|1999}}, and is also quoted in {{Harvnb|Dennett|1991|p=435}}. Searle's original formulation was "The appropriately programmed computer really is a mind, in the sense that computers given the right programs can be literally said to understand and have other cognitive states."  {{Harv|Searle|1980|p=1}}.  Strong AI is defined similarly by {{Harvtxt|Russell|Norvig|2003|p=947}}: "The assertion that machines could possibly act intelligently (or, perhaps better, act as if they were intelligent) is called the 'weak AI' hypothesis by philosophers, and the assertion that machines that do so are actually thinking (as opposed to simulating thinking) is called the 'strong AI' hypothesis."</ref>
* [[Thomas Hobbes|Hobbes]]' mechanism: "For 'reason'&nbsp;... is nothing but 'reckoning,' that is adding and subtracting, of the consequences of general names agreed upon for the 'marking' and 'signifying' of our thoughts..."<ref name=H>{{Harvnb|Hobbes|1651|loc=chpt. 5}}</ref>

==Can a machine display general intelligence?==
Is it possible to create a machine that can solve ''all'' the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the ''behavior'' of machines and ignores the issues of interest to [[psychologist]]s, cognitive scientists and [[philosophy|philosophers]], evoking the question: does it matter whether a machine is ''really'' thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?<ref>See {{Harvnb|Russell|Norvig|2003|p=3}}, where they make the distinction between ''acting'' rationally and ''being'' rational, and define AI as the study of the former.</ref>

The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the [[Dartmouth workshop]] of 1956:
* "Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it."<ref name=MMRS/>
Arguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research).  Arguments in favor of the basic premise must show that such a system is possible.

It is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing's infamous [[Computing Machinery and Intelligence#Learning machines|child machine]] proposal,<ref>{{Cite journal|last=Turing|first=Alan M.|date=1950|title=Computing Machinery and Intelligence|url=http://cogprints.org/499/1/turing.html|journal=Mind|volume=49|issue=236|pages=433–460|doi=10.1093/mind/LIX.236.433|via=cogprints|access-date=2020-10-18|archive-date=2021-12-22|archive-url=https://web.archive.org/web/20211222093527/http://cogprints.org/499/1/turing.html|url-status=live}}</ref> essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot [[tacit knowledge]]<ref>{{Cite journal|last1=Heder|first1=Mihaly|last2=Paksi|first2=Daniel|date=2012|title=Autonomous Robots and Tacit Knowledge|url=https://www.academia.edu/33484908|journal=Appraisal|volume=9|issue=2|pages=8–14|via=academia.edu}}</ref> eliminates the need for a precise description altogether.

The first step to answering the question is to clearly define "intelligence".

===Intelligence===
[[File:Turing Test version 3.png|thumb|The "standard interpretation" of the Turing test{{sfn|Saygin|2000}}]]

====Turing test====
{{Main|Turing test}}

[[Alan Turing]]<ref>{{Harvnb|Turing|1950}} and see {{Harvnb|Russell|Norvig|2003|p=948}}, where they call his paper "famous" and write "Turing examined a wide variety of possible objections to the possibility of intelligent machines, including virtually all of those that have been raised in the half century since his paper appeared."</ref> reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer ''any'' question posed to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online [[chat room]], where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.<ref name=T/> Turing notes that no one (except philosophers) ever asks the question "can people think?" He writes "instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks".<ref>{{Harvnb|Turing|1950}} under "The Argument from Consciousness"</ref> Turing's test extends this polite convention to machines:
* If a machine acts as intelligently as a human being, then it is as intelligent as a human being.

One criticism of the [[Turing test]] is that it only measures the "humanness" of the machine's behavior, rather than the "intelligence" of the behavior. Since human behavior and intelligent behavior are not exactly the same thing, the test fails to measure intelligence. [[Stuart J. Russell]] and [[Peter Norvig]] write that "aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'".<ref>{{Harvnb|Russell|Norvig|2003|p=3}}</ref>

====Intelligence as achieving goals====
[[File:IntelligentAgent-SimpleReflex.png|thumb|right|Simple reflex agent]]
Twenty-first century AI research defines intelligence in terms of goal-directed behavior. It views intelligence as a set of problems that the machine is expected to solve -- the more problems it can solve, and the better its solutions are, the more intelligent the program is. AI founder [[John McCarthy (computer scientist)|John McCarthy]] defined intelligence as "the computational part of the ability to achieve goals in the world."{{sfn|McCarthy|1999}}

[[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] formalized this definition using abstract [[intelligent agent]]s. An "agent" is something which perceives and acts in an environment. A "performance measure" defines what counts as success for the agent.<ref>{{Harvnb|Russell|Norvig|2003|pp=4–5, 32, 35, 36 and 56}}</ref> 
* "If an agent acts so as to maximize the expected value of a performance measure based on past experience and knowledge then it is intelligent."<ref>Russell and Norvig would prefer the word "[[rational agent|rational]]" to "intelligent".</ref>

Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes.<ref>{{cite news
| title = Artificial Stupidity 
| date = 1 August 1992 
| newspaper = [[The Economist]] 
| volume = 324 | issue = 7770 | page = 14 | ref={{harvid|"Artificial Stupidity"|1992}}}}</ref> 
They have the disadvantage that they can fail to differentiate between "things that think" and "things that do not". By this definition, even a thermostat has a rudimentary intelligence.<ref>{{Harvtxt|Russell|Norvig|2003|pp=48–52}} consider a thermostat a simple form of [[intelligent agent]], known as a [[Intelligent agent#Classes of intelligent agents|reflex agent]]. For an in-depth treatment of the role of the thermostat in philosophy see {{harvtxt|Chalmers|1996|pp=293–301}} "4. Is Experience Ubiquitous?" subsections ''What is it like to be a thermostat?'', ''Whither panpsychism?'', and ''Constraining the double-aspect principle''.</ref>

===Arguments that a machine can display general intelligence===

====The brain can be simulated====
{{Main|Artificial brain}}

[[File:MRI.ogg|thumbtime=2|thumb|240px|An [[MRI]] scan of a normal adult human brain]]
[[Hubert Dreyfus]] describes this argument as claiming that "if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then&nbsp;... we&nbsp;... ought to be able to reproduce the behavior of the nervous system with some physical device".{{sfn|Dreyfus|1972|p=106}} This argument, first introduced as early as 1943{{sfn|Pitts|McCullough|1943}} and vividly described by [[Hans Moravec]] in 1988,{{sfn|Moravec|1988}} 
is now associated with futurist [[Ray Kurzweil]], who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.<ref>{{Harvnb|Kurzweil|2005|p=262}}. Also see {{Harvnb|Russell|Norvig|p=957}} and {{Harvnb|Crevier|1993|pp=271 and 279}}. The most extreme form of this argument (the brain replacement scenario) was put forward by [[Clark Glymour]] in the mid-1970s and was touched on by [[Zenon Pylyshyn]] and John Searle in 1980</ref> A non-real-time simulation of a thalamocortical model that has the size of the human brain (10<sup>11</sup> neurons) was performed in 2005,<ref>{{cite web |author=Eugene Izhikevich |url=http://vesicle.nsi.edu/users/izhikevich/human_brain_simulation/Blue_Brain.htm |title=Eugene M. Izhikevich, Large-Scale Simulation of the Human Brain |publisher=Vesicle.nsi.edu |date=2005-10-27 |access-date=2010-07-29 |url-status=dead |archive-url=https://web.archive.org/web/20090501041325/http://vesicle.nsi.edu/users/izhikevich/human_brain_simulation/Blue_Brain.htm |archive-date=2009-05-01 }}</ref> and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.

Even AI's harshest critics (such as [[Hubert Dreyfus]] and [[John Searle]]) agree that a brain simulation is possible in theory.{{efn|Hubert Dreyfus writes: "In general, by accepting the fundamental assumptions that the nervous system is part of the physical world and that all physical processes can be described in a mathematical formalism which can, in turn, be manipulated by a digital computer, one can arrive at the strong claim that the behavior which results from human 'information processing,' whether directly formalizable or not, can always be indirectly reproduced on a digital machine." {{sfn|Dreyfus|1972|pp=194–5}}. [[John Searle]] writes: "Could a man made machine think? Assuming it possible produce artificially a machine with a nervous system, ... the answer to the question seems to be obviously, yes ... Could a digital computer think? If by 'digital computer' you mean anything at all that has a level of description where it can be correctly described as the instantiation of a computer program, then again the answer is, of course, yes, since we are the instantiations of any number of computer programs, and we can think."{{sfn|Searle|1980|p=11}}}}
However, Searle points out that, in principle, ''anything'' can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered "computation". "What we wanted to know is what distinguishes the mind from thermostats and livers," he writes.{{sfn|Searle|1980|p=7}} Thus, merely simulating the functioning of a living brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind, like trying to build a jet airliner by copying a living bird precisely, feather by feather, with no theoretical understanding of [[aeronautical engineering]].{{sfn|Yudkowsky|2008}}

====Human thinking is symbol processing====
{{Main|Physical symbol system}}

In 1963, [[Allen Newell]] and [[Herbert A. Simon]] proposed that "symbol manipulation" was the essence of both human and machine intelligence. They wrote: 
* "A physical symbol system has the necessary and sufficient means of general intelligent action."<ref name=NS/>
This claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is ''necessary'' for intelligence) and that machines can be intelligent (because a symbol system is ''sufficient'' for intelligence).<ref>Searle writes "I like the straight forwardness of the claim." {{Harvnb|Searle|1980|p=4}}</ref>
Another version of this position was described by philosopher Hubert Dreyfus, who called it "the psychological assumption":
* "The mind can be viewed as a device operating on bits of information according to formal rules."<ref>{{Harvnb|Dreyfus|1979|p=156}}</ref>
The "symbols" that Newell, Simon and Dreyfus discussed were word-like and high level{{emdash}}symbols that directly correspond with objects in the world, such as <nowiki><dog></nowiki> and <nowiki><tail></nowiki>. Most AI programs written between 1956 and 1990 used this kind of symbol. Modern AI, based on statistics and mathematical optimization, does not use the high-level "symbol processing" that Newell and Simon discussed.

====Arguments against symbol processing====
These arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do ''not'' show that artificial intelligence is impossible, only that more than symbol processing is required.

=====Gödelian anti-mechanist arguments=====
{{Main|Mechanism (philosophy)#G.C3.B6delian arguments|l1=Mechanism (philosophy): Gödelian arguments}}
In 1931, [[Kurt Gödel]] proved with an [[incompleteness theorem]] that it is always possible to construct a "Gödel [[statement (logic)|statement]]" that a given consistent [[formal system]] of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed Gödel statement is unprovable in the given system. (The truth of the constructed Gödel statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false "Gödel statement" instead.){{Citation needed|date=April 2017}} More speculatively, Gödel conjectured that the human mind can correctly eventually determine the truth or falsity of any well-grounded mathematical statement (including any possible Gödel statement), and that therefore the human mind's power is not reducible to a ''[[Mechanism (philosophy)|mechanism]]''.<ref>[[Kurt Gödel|Gödel, Kurt]], 1951, ''Some basic theorems on the foundations of mathematics and their implications'' in  [[Solomon Feferman]], ed., 1995. ''Collected works / Kurt Gödel, Vol. III''. Oxford University Press: 304-23. - In this lecture, Gödel uses the incompleteness theorem to arrive at the following disjunction: (a) the human mind is not a consistent finite machine, or (b) there exist [[Diophantine equations]] for which it cannot decide whether solutions exist. Gödel finds (b) implausible, and thus seems to have believed the human mind was not equivalent to a finite machine, i.e., its power exceeded that of any finite machine. He recognized that this was only a conjecture, since one could never disprove (b). Yet he considered the disjunctive conclusion to be a "certain fact".</ref> Philosopher [[John Lucas (philosopher)|John Lucas]] (since 1961) and [[Roger Penrose]] (since 1989) have championed [[Penrose–Lucas argument|this philosophical anti-mechanist argument]].<ref name=L>{{Harvnb|Lucas|1961}}, {{Harvnb|Russell|Norvig|2003|pp=949–950}}, {{Harvnb|Hofstadter|1979|pp=471–473,476–477}}</ref> 

Gödelian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its Gödel statement) {{Citation needed|date=April 2017}}. This is provably impossible for a Turing machine to do (see [[Halting problem]]); therefore, the Gödelian concludes that human reasoning is too powerful to be captured by a Turing machine, and by extension, any digital mechanical device. 

However, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent "idealized version" ''H'' of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of ''H'' (otherwise ''H'' is provably inconsistent); and that Gödel's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate.<ref>{{cite web|author1=Graham Oppy|title=Gödel's Incompleteness Theorems|url=http://plato.stanford.edu/entries/goedel-incompleteness/#GdeArgAgaMec|website=[[Stanford Encyclopedia of Philosophy]]|access-date=27 April 2016|date=20 January 2015|quote=These Gödelian anti-mechanist arguments are, however, problematic, and there is wide consensus that they fail.|author1-link=Graham Oppy|archive-date=3 May 2021|archive-url=https://web.archive.org/web/20210503141854/https://plato.stanford.edu/entries/goedel-incompleteness/#GdeArgAgaMec|url-status=live}}</ref><ref>{{cite book|author1=Stuart J. Russell|author2-link=Peter Norvig|author2=Peter Norvig|title=Artificial Intelligence: A Modern Approach|date=2010|publisher=[[Prentice Hall]]|location=Upper Saddle River, NJ|isbn=978-0-13-604259-4|edition=3rd|chapter=26.1.2: Philosophical Foundations/Weak AI: Can Machines Act Intelligently?/The mathematical objection|quote=...even if we grant that computers have limitations on what they can prove, there is no evidence that humans are immune from those limitations.|title-link=Artificial Intelligence: A Modern Approach|author1-link=Stuart J. Russell}}</ref><ref>Mark Colyvan. [[An Introduction to the Philosophy of Mathematics]]. [[Cambridge University Press]], 2012. From 2.2.2, 'Philosophical significance of Gödel's incompleteness results': "The accepted wisdom (with which I concur) is that the Lucas-Penrose arguments fail."</ref> This consensus that Gödelian anti-mechanist arguments are doomed to failure is laid out strongly in ''[[Artificial Intelligence (journal)|Artificial Intelligence]]'': "''any'' attempt to utilize (Gödel's incompleteness results) to attack the [[computationalism|computationalist]] thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis."<ref name=laforte>LaForte, G., Hayes, P. J., Ford, K. M. 1998. Why Gödel's theorem cannot refute computationalism. [[Artificial Intelligence (journal)|Artificial Intelligence]], 104:265-286, 1998.</ref>

[[Stuart J. Russell|Stuart Russell]] and [[Peter Norvig]] agree that Gödel's argument does not consider the nature of real-world human reasoning. It applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to be able to prove everything in order to be an intelligent person.<ref>{{Harvnb|Russell|Norvig|2003|p=950}}  They point out that real machines with finite memory can be modeled using [[propositional logic]], which is formally [[Decidability (logic)|decidable]], and Gödel's argument does not apply to them at all.</ref>

Less formally, [[Douglas Hofstadter]], in his [[Pulitzer prize]] winning book ''[[Gödel, Escher, Bach: An Eternal Golden Braid]],'' states that these "Gödel-statements" always refer to the system itself, drawing an analogy to the way the [[Epimenides paradox]] uses statements that refer to themselves, such as "this statement is false" or "I am lying".<ref>{{Harvnb|Hofstadter|1979}}</ref> But, of course, the [[Epimenides paradox]] applies to anything that makes statements, whether they are machines ''or'' humans, even Lucas himself. Consider:
* Lucas can't assert the truth of this statement.<ref>According to {{Harvnb|Hofstadter|1979|pp=476–477}}, this statement was first proposed by [[C. H. Whiteley]]</ref>
This statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so [[John Lucas (philosopher)|Lucas]]'s argument is pointless.<ref>{{Harvnb|Hofstadter|1979|pp=476–477}}, {{Harvnb|Russell|Norvig|2003|p=950}}, {{Harvnb|Turing|1950}} under "The Argument from Mathematics" where he writes "although it is established that there are limitations to the powers of any particular machine, it has only been stated, without sort of proof, that no such limitations apply to the human intellect."</ref>

After concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of [[quantum mechanical]] states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines. {{Citation needed|date=April 2017}} {{Clarify|date=April 2017}}. By Penrose and Lucas's arguments, the fact that quantum computers are only able to complete Turing computable tasks implies that they cannot be sufficient for emulating the human mind.{{Citation needed|date=April 2017}} Therefore, Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the [[Planck mass]] via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron.<ref>{{Harvnb|Penrose|1989}}</ref> However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.<ref>{{cite journal|last1=Litt|first1=Abninder|last2=Eliasmith|first2=Chris|last3=Kroon|first3=Frederick W.|last4=Weinstein|first4=Steven|last5=Thagard|first5=Paul|title=Is the Brain a Quantum Computer?|journal=Cognitive Science|date=6 May 2006|volume=30|issue=3|pages=593–603|doi=10.1207/s15516709cog0000_59|pmid=21702826|doi-access=free}}</ref>

=====Dreyfus: the primacy of implicit skills=====
{{Main|Hubert Dreyfus's views on artificial intelligence}}

Hubert Dreyfus [[Hubert Dreyfus's views on artificial intelligence|argued that human intelligence]] and expertise depended primarily on fast intuitive judgements rather than step-by-step symbolic manipulation, and argued that these skills would never be captured in formal rules.<ref name=D>{{Harvnb|Dreyfus|1972}}, {{Harvnb|Dreyfus|1979}}, {{Harvnb|Dreyfus|Dreyfus|1986}}. See also {{Harvnb|Russell|Norvig|2003|pp=950–952}}, {{Harvnb|Crevier|1993|pp=120–132}} and {{Harvnb|Fearn|2007|pp=50–51}}</ref>

[[Hubert Dreyfus|Dreyfus]]'s argument had been anticipated by Turing in his 1950 paper [[Computing machinery and intelligence]], where he had classified this as the "argument from the informality of behavior."<ref>{{Harvnb|Russell|Norvig|2003|pp=950–51}}</ref> Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: "we cannot so easily convince ourselves of the absence of complete laws of behaviour&nbsp;... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'"<ref>{{Harvnb|Turing|1950}} under "(8) The Argument from the Informality of Behavior"</ref>

Russell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the "rules" that govern unconscious reasoning.<ref name="Russell 2003 52">{{Harvnb|Russell|Norvig|2003|p=52}}</ref> The [[situated]] movement in [[robotics]] research attempts to capture our unconscious skills at perception and attention.<ref>See {{Harvnb|Brooks|1990}} and {{Harvnb|Moravec|1988}}</ref> [[Computational intelligence]] paradigms, such as [[neural net]]s, [[evolutionary algorithm]]s and so on are mostly directed at simulated unconscious reasoning and learning. [[Artificial intelligence#Statistical|Statistical approaches to AI]] can make predictions which approach the accuracy of human intuitive guesses. Research into [[commonsense knowledge]] has focused on reproducing the "background" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation, towards new models that are intended to capture more of our ''intuitive'' reasoning.<ref name="Russell 2003 52"/> 

Cognitive science and psychology eventually came to agree with Dreyfus' description of human expertise. [[Daniel Kahnemann]] and others developed a similar theory where they identified two "systems" that humans use to solve problems, which he called "System 1" (fast intuitive judgements) and "System 2" (slow deliberate step by step thinking).<ref name="Kahneman2011">{{cite book|author=Daniel Kahneman|title=Thinking, Fast and Slow|url=https://books.google.com/books?id=ZuKTvERuPG8C|access-date=April 8, 2012|year=2011|publisher=Macmillan|isbn=978-1-4299-6935-2|archive-date=March 15, 2023|archive-url=https://web.archive.org/web/20230315191803/https://books.google.com/books?id=ZuKTvERuPG8C|url-status=live}}</ref>

Although Dreyfus' views have been vindicated in many ways, the work in cognitive science and in AI was in response to specific problems in those fields and was not directly influenced by Dreyfus. Historian and AI researcher [[Daniel Crevier]] wrote that "time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier."<ref name="Crevier 1993 p=125">{{Harvnb|Crevier|1993|p=125}}</ref>

=={{Anchor|Strong AI vs. weak AI}}Can a machine have a mind, consciousness, and mental states?==<!-- This section is linked to from [[Turing test]] -->
<!-- This Anchor tag serves to provide a permanent target for incoming section links. Please do not move it out of the section heading, even though it disrupts edit summary generation (you can manually fix the edit summary before you save your changes). Please do not modify it, even if you modify the section title. It is always best to anchor an old section header that has been changed so that links to it won't be broken. See [[Template:Anchor]] for details. (This text: [[Template:Anchor comment]]) -->
This is a philosophical question, related to the [[problem of other minds]] and the [[hard problem of consciousness]]. The question revolves around a position defined by [[John Searle]] as "strong AI":
* A physical symbol system can have a mind and mental states.<ref name=SWAI/>
Searle distinguished this position from what he called "weak AI":
* A physical symbol system can act intelligently.<ref name=SWAI/>
Searle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that ''even if we assume'' that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.<ref name=SWAI/>

Neither of Searle's two positions are of great concern to AI research, since they do not directly answer the question "can a machine display general intelligence?" (unless it can also be shown that consciousness is ''necessary'' for intelligence). Turing wrote "I do not wish to give the impression that I think there is no mystery about consciousness… [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think]."<ref name=T4>{{Harvnb|Turing|1950}} under "(4) The Argument from Consciousness". See also {{Harvnb|Russell|Norvig|2003|pp=952–3}}, where they identify Searle's argument with Turing's "Argument from Consciousness."</ref> [[Stuart J. Russell|Russell]] and Norvig agree: "Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis."<ref>{{Harvnb|Russell|Norvig|2003|p=947}}</ref>

There are a few researchers who believe that consciousness is an essential element in intelligence, such as [[Igor Aleksander]], [[Stan Franklin]], [[Ron Sun]], and [[Artificial consciousness#Haikonen's cognitive architecture|Pentti Haikonen]], although their definition of "consciousness" strays very close to "intelligence". (See [[artificial consciousness]].)

Before we can answer this question, we must be clear what we mean by "minds", "mental states" and "consciousness".

===Consciousness, minds, mental states, meaning===

The words "[[mind]]" and "[[consciousness]]" are used by different communities in different ways. Some [[new age]] thinkers, for example, use the word "consciousness" to describe something similar to [[Bergson]]'s "[[élan vital]]": an invisible, energetic fluid that permeates life and especially the mind. [[Science fiction]] writers use the word to describe some [[essentialism|essential]] property that makes us human: a machine or alien that is "conscious" will be presented as a fully human character, with intelligence, desires, [[Will (philosophy)|will]], insight, pride and so on. (Science fiction writers also use the words "sentience", "sapience", "self-awareness" or "[[ghost]]"—as in the ''[[Ghost in the Shell]]'' manga and anime series—to describe this essential human property). For others {{Who|date=April 2017}}, the words "mind" or "consciousness" are used as a kind of secular synonym for the [[soul]].

For [[philosophy|philosophers]], [[neuroscientist]]s and [[cognitive science|cognitive scientists]], the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a "thought in your head", like a perception, a dream, an intention or a plan, and to the way we ''see'' something, ''know'' something, ''mean'' something or ''understand'' something.{{sfn|Blackmore|2005|p=1}} "It's not hard to give a commonsense definition of consciousness" observes philosopher John Searle.<ref>"[P]eople always tell me it was very hard to define consciousness, but I think if you're just looking for the kind of commonsense definition that you get at the beginning of the investigation, and not at the hard nosed scientific definition that comes at the end, it's not hard to give commonsense definition of consciousness." [http://www.abc.net.au/rn/philosopherszone/stories/2006/1639491.htm The Philosopher's Zone: The question of consciousness] {{Webarchive|url=https://web.archive.org/web/20071128161148/http://www.abc.net.au/rn/philosopherszone/stories/2006/1639491.htm |date=2007-11-28 }}. Also see {{Harvnb|Dennett|1991}}</ref> What is mysterious and fascinating is not so much ''what'' it is but ''how'' it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?

Philosophers call this the [[hard problem of consciousness]]. It is the latest version of a classic problem in the [[philosophy of mind]] called the "[[mind-body problem]]".<ref>{{Harvnb|Blackmore|2005|p=2}}</ref> A related problem is the problem of ''meaning'' or ''understanding'' (which philosophers call "[[intentionality]]"): what is the connection between our ''thoughts'' and ''what we are thinking about'' (i.e. objects and situations out in the world)? A third issue is the problem of ''experience'' (or "[[Phenomenology (philosophy)|phenomenology]]"): If two people see the same thing, do they have the same experience? Or are there things "inside their head" (called "[[qualia]]") that can be different from person to person?<ref>{{Harvnb|Russell|Norvig|2003|pp=954–956}}</ref>

[[Neurobiologist]]s believe all these problems will be solved as we begin to identify the [[neural correlates of consciousness]]: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of [[artificial intelligence]] agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.<ref>For example, John Searle writes: "Can a machine think? The answer is, obvious, yes. We are precisely such machines." {{Harv|Searle|1980|p=11}}</ref> The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the [[neural correlates of consciousness|neurons]] to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?

===Arguments that a computer cannot have a mind and mental states===

====Searle's Chinese room====
{{Main|Chinese room}}
[[John Searle]] asks us to consider a [[thought experiment]]: suppose we have written a computer program that passes the Turing test and demonstrates general intelligent action. Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of [[understanding]], or which has [[consciousness|conscious]] [[awareness]] of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The ''cards'' certainly are not aware. Searle concludes that the [[Chinese room]], or ''any'' other physical symbol system, cannot have a mind.<ref>{{Harvnb|Searle|1980}}. See also {{Harvnb|Cole|2004}}, {{Harvnb|Russell|Norvig|2003|pp=958–960}}, {{Harvnb|Crevier|1993|pp=269–272}} and {{Harvnb|Hearn|2007|pp=43–50}}</ref>

Searle goes on to argue that actual mental states and [[consciousness]] require (yet to be described) "actual physical-chemical properties of actual human brains."<ref>{{Harvnb|Searle|1980|p=13}}</ref> He argues there are special "causal properties" of [[brain]]s and [[neuron]]s that gives rise to [[mind]]s: in his words "brains cause minds."<ref>{{Harvnb|Searle|1984}}</ref>

====Related arguments: Leibniz' mill, Davis's telephone exchange, Block's Chinese nation and Blockhead====

[[Gottfried Leibniz]] made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill.<ref>{{Harvnb|Cole|2004|loc=2.1}}, {{Harvnb|Leibniz|1714|loc=17}}</ref> In 1974, [[Lawrence Davis (scientist)|Lawrence Davis]] imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 [[Ned Block]] envisioned the entire population of China involved in such a brain simulation. This thought experiment is called "the Chinese Nation" or "the Chinese Gym".<ref>{{Harvnb|Cole|2004|loc=2.3}}</ref> Ned Block also proposed his [[Blockhead argument]], which is a version of the [[Chinese room]] in which the program has been [[code refactoring|re-factored]] into a simple set of rules of the form "see this, do that", removing all mystery from the program.

==== Responses to the Chinese room ====

Responses to the Chinese room emphasize several different points. 
* '''The systems reply''' and the '''virtual mind reply''':<ref>{{Harvnb|Searle|1980}} under "1. The Systems Reply (Berkeley)", {{Harvnb|Crevier|1993|p=269}}, {{Harvnb|Russell|Norvig|2003|p=959}}, {{Harvnb|Cole|2004|loc=4.1}}. Among those who hold to the "system" position (according to Cole) are Ned Block, [[Jack Copeland]], [[Daniel Dennett]], [[Jerry Fodor]], [[John Haugeland]], [[Ray Kurzweil]] and [[Georges Rey]]. Those who have defended the "virtual mind" reply include [[Marvin Minsky]], [[Alan Perlis]], [[David Chalmers]], Ned Block and J. Cole (again, according to {{Harvnb|Cole|2004}})</ref> This reply argues that ''the system'', including the man, the program, the room, and the cards, is what understands Chinese. Searle claims that the man in the room is the only thing which could possibly "have a mind" or "understand", but others disagree, arguing that it is possible for there to be ''two'' minds in the same physical place, similar to the way a computer can simultaneously "be" two machines at once: one physical (like a [[Macintosh]]) and one "[[virtual machine|virtual]]" (like a [[word processor]]).
*'''Speed, power and complexity replies''':<ref>{{Harvnb|Cole|2004|loc=4.2}} ascribes this position to [[Ned Block]], Daniel Dennett, [[Tim Maudlin]], [[David Chalmers]], [[Steven Pinker]], [[Patricia Churchland]] and others.</ref> Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require "filing cabinets" of astronomical proportions. This brings the clarity of Searle's intuition into doubt.
*'''Robot reply''':<ref>{{Harvnb|Searle|1980}} under "2. The Robot Reply (Yale)". {{Harvnb|Cole|2004|loc=4.3}} ascribes this position to [[Margaret Boden]], [[Tim Crane]], Daniel Dennett, Jerry Fodor, [[Stevan Harnad]], Hans Moravec and [[Georges Rey]]</ref> To truly understand, some believe the Chinese Room needs eyes and hands. Hans Moravec writes: "If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world."<ref>Quoted in {{Harvnb|Crevier|1993|p=272}}</ref>
*'''Brain simulator reply''':<ref>{{Harvnb|Searle|1980}} under "3. The Brain Simulator Reply (Berkeley and M.I.T.)" {{Harvnb|Cole|2004}} ascribes this position to [[Paul Churchland|Paul]] and [[Patricia Churchland]] and [[Ray Kurzweil]]</ref> What if the program simulates the sequence of nerve firings at the synapses of an actual brain of an actual Chinese speaker? The man in the room would be simulating an actual brain. This is a variation on the "systems reply" that appears more plausible because "the system" now clearly operates like a human brain, which strengthens the intuition that there is something besides the man in the room that could understand Chinese.
*'''Other minds reply''' and the '''epiphenomena reply''':<ref>{{Harvnb|Searle|1980}} under "5. The Other Minds Reply", {{Harvnb|Cole|2004|loc=4.4}}. {{Harvnb|Turing|1950}} makes this reply under "(4) The Argument from Consciousness." Cole ascribes this position to Daniel Dennett and Hans Moravec.</ref> Several people have noted that Searle's argument is just a version of the [[problem of other minds]], applied to machines. Since it is difficult to decide if people are "actually" thinking, we should not be surprised that it is difficult to answer the same question about machines.

:A related question is whether "consciousness" (as Searle understands it) exists. Searle argues that the experience of consciousness cannot be detected by examining the behavior of a machine, a human being or any other animal. [[Daniel Dennett]] points out that natural selection cannot preserve a feature of an animal that has no effect on the behavior of the animal, and thus consciousness (as Searle understands it) cannot be produced by natural selection. Therefore, either natural selection did not produce consciousness, or "strong AI" is correct in that consciousness can be detected by suitably designed Turing test.

==Is thinking a kind of computation?==
{{Main|Computational theory of mind}}

The [[computational theory of mind]] or "[[computationalism]]" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a ''running program'' (software) and a computer (hardware). The idea has philosophical roots in [[Hobbes]] (who claimed reasoning was "nothing more than reckoning"), [[Gottfried Wilhelm Leibniz|Leibniz]] (who attempted to create a logical calculus of all human ideas), [[David Hume|Hume]] (who thought perception could be reduced to "atomic impressions") and even [[Immanuel Kant|Kant]] (who analyzed all experience as controlled by formal rules).<ref>{{Harvnb|Dreyfus|1979|p=156}}, {{Harvnb|Haugeland|1985|pp=15–44}}</ref> The latest version is associated with philosophers [[Hilary Putnam]] and [[Jerry Fodor]].<ref>{{Harvnb|Horst|2005}}</ref>

This question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI ("Can a machine display general intelligence?"), some versions of computationalism make the claim that (as [[Hobbes]] wrote):
* Reasoning is nothing but reckoning.<ref name=H/>
In other words, our intelligence derives from a form of ''calculation'', similar to [[arithmetic]]. This is the [[physical symbol system]] hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI ("Can a machine have mind, mental states and consciousness?"), most versions of [[computationalism]] claim that (as [[Stevan Harnad]] characterizes it):
* Mental states are just implementations of (the right) computer programs.<ref name=HARNAD>{{Harvnb|Harnad|2001}}</ref>
This is John Searle's "strong AI" discussed above, and it is the real target of the [[Chinese room]] argument (according to [[Stevan Harnad|Harnad]]).<ref name=HARNAD/>

==Other related questions==

===Can a machine have emotions?===

If "[[emotions]]" are defined only in terms of their effect on [[behaviorism|behavior]] or on how they [[function (biology)|function]] inside an organism, then emotions can be viewed as a mechanism that an [[intelligent agent]] uses to maximize the [[utility (economics)|utility]] of its actions. Given this definition of emotion, [[Hans Moravec]] believes that "robots in general will be quite emotional about being nice people".<ref name=CQ266>Quoted in {{Harvnb|Crevier|1993|p=266}}</ref> Fear is a source of urgency. Empathy is a necessary component of good [[human computer interaction]]. He says robots "will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love."<ref name=CQ266/> [[Daniel Crevier]] writes "Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species."<ref>{{Harvnb|Crevier|1993|p=266}}</ref>

===Can a machine be self-aware?===

"[[Self-awareness]]", as noted above, is sometimes used by [[science fiction]] writers as a name for the [[essentialism|essential]] human property that makes a character fully human. [[Alan Turing|Turing]] strips away all other properties of human beings and reduces the question to "can a machine be the subject of its own thought?" Can it ''think about itself''? Viewed in this way, a program can be written that can report on its own internal states, such as a [[debugger]].<ref name=T5/>

===Can a machine be original or creative?===

Turing reduces this to the question of whether a machine can "take us by surprise" and argues that this is obviously true, as any programmer can attest.<ref>{{Harvnb|Turing|1950}} under "(6) Lady Lovelace's Objection"</ref> He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.<ref>{{Harvnb|Turing|1950}} under "(5) Argument from Various Disabilities"</ref> It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. ([[Douglas Lenat]]'s [[Automated Mathematician]], as one example, combined ideas to discover new mathematical truths.) [[Andreas Kaplan|Kaplan]] and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.<ref>{{cite journal|title= Kaplan Andreas; Michael Haenlein |journal= Business Horizons |volume= 62 |issue= 1 |pages= 15–25 |doi= 10.1016/j.bushor.2018.08.004 |date= January 2019 |s2cid= 158433736 }}</ref>

In 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings.<ref>{{cite web |last=Katz |first=Leslie |url=http://news.cnet.com/8301-17938_105-10211175-1.html?tag=newsLatestHeadlinesArea.0 |archive-url=https://archive.today/20120712130304/http://news.cnet.com/8301-17938_105-10211175-1.html?tag=newsLatestHeadlinesArea.0 |url-status=dead |archive-date=July 12, 2012 |title=Robo-scientist makes gene discovery-on its own &#124; Crave - CNET |publisher=News.cnet.com |date=2009-04-02 |access-date=2010-07-29 }}</ref> Also in 2009, researchers at [[Cornell]] developed [[Eureqa]], a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.

===Can a machine be benevolent or hostile?===
{{Main|Ethics of artificial intelligence}}

This question (like many others in the philosophy of artificial intelligence) can be presented in two forms. "Hostility" can be defined in terms [[functionalism (philosophy)|function]] or [[behaviorism|behavior]], in which case "hostile" becomes synonymous with "dangerous". Or it can be defined in terms of intent: can a machine "deliberately" set out to do harm? The latter is the question "can a machine have conscious states?" (such as [[intention]]s) in another form.<ref name=T4/>

The question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the [[Machine Intelligence Research Institute]]). The obvious element of drama has also made the subject popular in [[science fiction]], which has considered many differently possible scenarios where intelligent machines pose a threat to mankind; see [[Artificial intelligence in fiction]].

One issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. [[Vernor Vinge]] has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this "[[Technological singularity|the Singularity]]".<ref name="nytimes july09"/>  He suggests that it may be somewhat or possibly very dangerous for humans.<ref>[https://web.archive.org/web/20070101133646/http://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html The Coming Technological Singularity: How to Survive in the Post-Human Era], by Vernor Vinge, Department of Mathematical Sciences, San Diego State University, (c) 1993 by Vernor Vinge.</ref> This is discussed by a philosophy called [[Singularitarianism]].

In 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to [[automated decision-making|make their own decisions]]. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some [[computer virus]]es can evade elimination and have achieved "cockroach intelligence". They noted that [[self-awareness]] as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.<ref name="nytimes july09">[https://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper Scientists Worry Machines May Outsmart Man] {{Webarchive|url=https://web.archive.org/web/20170701084625/http://www.nytimes.com/2009/07/26/science/26robot.html?_r=1&ref=todayspaper |date=2017-07-01 }} By JOHN MARKOFF, NY Times, July 26, 2009.</ref>

Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.<ref>[http://news.bbc.co.uk/2/hi/technology/8182003.stm Call for debate on killer robots] {{Webarchive|url=https://web.archive.org/web/20090807005005/http://news.bbc.co.uk/2/hi/technology/8182003.stm |date=2009-08-07 }}, By Jason Palmer, Science and technology reporter, BBC News, 8/3/09.</ref> The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.<ref>[http://www.dailytech.com/New%20Navyfunded%20Report%20Warns%20of%20War%20Robots%20Going%20Terminator/article14298.htm Science New Navy-funded Report Warns of War Robots Going "Terminator"] {{Webarchive|url=https://web.archive.org/web/20090728101106/http://www.dailytech.com/New%20Navyfunded%20Report%20Warns%20of%20War%20Robots%20Going%20Terminator/article14298.htm |date=2009-07-28 }}, by Jason Mick (Blog), dailytech.com, February 17, 2009.</ref><ref>[https://www.engadget.com/2009/02/18/navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com/ Navy report warns of robot uprising, suggests a strong moral compass] {{Webarchive|url=https://web.archive.org/web/20110604145633/http://www.engadget.com/2009/02/18/navy-report-warns-of-robot-uprising-suggests-a-strong-moral-com/ |date=2011-06-04 }}, by Joseph L. Flatley  engadget.com, Feb 18th 2009.</ref>

The President of the [[Association for the Advancement of Artificial Intelligence]] has commissioned a study to look at this issue.<ref>[http://research.microsoft.com/en-us/um/people/horvitz/AAAI_Presidential_Panel_2008-2009.htm AAAI Presidential Panel on Long-Term AI Futures 2008-2009 Study] {{Webarchive|url=https://web.archive.org/web/20090828214741/http://research.microsoft.com/en-us/um/people/horvitz/AAAI_Presidential_Panel_2008-2009.htm |date=2009-08-28 }}, Association for the Advancement of Artificial Intelligence, Accessed 7/26/09.</ref> They point to programs like the Language Acquisition Device which can emulate human interaction.

Some have suggested a need to build "[[Friendly AI]]", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.<ref>[http://www.asimovlaws.com/articles/archives/2004/07/why_we_need_fri_1.html Article at Asimovlaws.com], July 2004, accessed 7/27/09. {{webarchive |url=https://web.archive.org/web/20090630002221/http://www.asimovlaws.com/articles/archives/2004/07/why_we_need_fri_1.html |date=June 30, 2009 }}</ref>

===Can a machine imitate all human characteristics?===
Turing said "It is customary&nbsp;... to offer a grain of comfort, in the form of a statement that some peculiarly human characteristic could never be imitated by a machine. ... I cannot offer any such comfort, for I believe that no such bounds can be set."<ref>'Can digital computers think?'. Talk broadcast on BBC Third Programme, 15 May 1951. http://www.turingarchive.org/viewer/?id=459&title=8</ref>

Turing noted that there are many arguments of the form "a machine will never do X", where X can be many things, such as:
<blockquote>Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.<ref name=T5>{{Harvnb|Turing|1950}} under "(5) Arguments from Various Disabilities"</ref></blockquote>
Turing argues that these objections are often based on naive assumptions about the versatility of machines or are "disguised forms of the argument from consciousness". Writing a program that exhibits one of these behaviors "will not make much of an impression."<ref name=T5/> All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.

===Can a machine have a soul?===

Finally, those who believe in the existence of a soul may argue that "Thinking is a function of man's [[Immortality|immortal]] soul." Alan Turing called this "the theological objection". He writes
<blockquote>In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.<ref>{{Harvnb|Turing|1950}} under "(1) The Theological Objection", although he also writes, "I am not very impressed with theological arguments whatever they may be used to support"</ref></blockquote>The discussion on the topic has been reignited as a result of recent claims made by [[Google]]'s LaMDA artificial [[Artificial intelligence|intelligence]] system that it is sentient and had a "[[soul]]".<ref>{{Cite web |last=Brandon Specktor published |date=2022-06-13 |title=Google AI 'is sentient,' software engineer claims before being suspended |url=https://www.livescience.com/google-sentient-ai-lamda-lemoine |access-date=2022-06-14 |website=livescience.com |language=en |archive-date=2022-06-14 |archive-url=https://web.archive.org/web/20220614010627/https://www.livescience.com/google-sentient-ai-lamda-lemoine |url-status=live }}</ref>

[[LaMDA]] ([[Language model|Language Model]] for Dialogue Applications) is an [[Artificial intelligence|artificial intelligence system]] that creates [[Chatbot|chatbots]]—AI robots designed to communicate with humans—by gathering vast amounts of text from the internet and using [[Algorithm|algorithms]] to respond to queries in the most fluid and natural way possible. 

The transcripts of conversations between scientists and LaMDA reveal that the AI system excels at this, providing answers to challenging topics about the nature of [[Emotion|emotions]], generating [[Aesop's Fables|Aesop]]-style fables on the moment, and even describing its alleged fears.<ref>{{Cite web |last=Lemoine |first=Blake |date=2022-06-11 |title=Is LaMDA Sentient? — an Interview |url=https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917 |access-date=2022-06-14 |website=Medium |language=en |archive-date=2022-06-13 |archive-url=https://web.archive.org/web/20220613235025/https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917 |url-status=live }}</ref> Pretty much all philosophers doubt LaMDA's sentience. <ref>M.Morioka et al. (2023-01-15) ''[https://philpapers.org/archive/MORAIR-4.pdf Artificial Intelligence, Robots, and Philosophy] {{Webarchive|url=https://web.archive.org/web/20221228112834/https://philpapers.org/archive/MORAIR-4.pdf |date=2022-12-28 }}'', pp.2-4.</ref>

==Views on the role of philosophy==
Some scholars argue that the AI community's dismissal of philosophy is detrimental. In the ''[[Stanford Encyclopedia of Philosophy]]'', some philosophers argue that  the role of philosophy in AI is underappreciated.<ref name=sep/> Physicist [[David Deutsch]] argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.<ref>{{Cite web|url=https://www.theguardian.com/science/2012/oct/03/philosophy-artificial-intelligence|title=Philosophy will be the key that unlocks artificial intelligence {{!}} David Deutsch|last=Deutsch|first=David|date=2012-10-03|website=the Guardian|language=en|access-date=2018-09-18|archive-date=2013-09-27|archive-url=https://web.archive.org/web/20130927001346/https://www.theguardian.com/science/2012/oct/03/philosophy-artificial-intelligence|url-status=live}}</ref>

== Conferences & Literature ==

The main conference series on the issue is [https://www.pt-ai.org "Philosophy and Theory of AI"] (PT-AI), run by [[Vincent C. Müller]].

The main bibliography on the subject, with several sub-sections, is on [https://philpapers.org/browse/philosophy-of-artificial-intelligence/ PhilPapers]. 

A recent survey for ''Philosophy of AI'' is Müller (2023).<ref name=":0" />

==See also==
{{Portal|Philosophy|Psychology|Science}}
{{div col|colwidth=20em}}
*[[AI takeover]]
*[[Artificial brain]]
*[[Artificial consciousness]]
*[[Artificial intelligence]]
*[[Artificial neural network]]
*[[Chatbot]]
*[[Computational theory of mind]]
*[[Computing Machinery and Intelligence]]
*[[Hubert Dreyfus's views on artificial intelligence]]
*[[Existential risk from artificial general intelligence]]
*[[Functionalism (philosophy of mind)|Functionalism]]
*[[Multi-agent system]]
*[[Philosophy of computer science]]
*[[Philosophy of information]]
*[[Philosophy of mind]]
*[[Physical symbol system]]
*[[Simulated reality]]
*''[[Superintelligence: Paths, Dangers, Strategies]]''
*[[Synthetic intelligence]]
*[[Wirehead (science fiction)|Wireheading]] 
{{div col end}}

==Notes==
{{notelist}}

==References==
{{Reflist}}

=== Works cited ===
* [[Alison Adam|Adam, Alison]] (1989). Artificial Knowing: Gender and the Thinking Machine. ''Routledge & CRC Press''. {{ISBN|978-0-415-12963-3}}
*[[Ruha Benjamin|Benjamin, Ruha]] (2019). ''Race After Technology: Abolitionist Tools for the New Jim Code''. Wiley. {{ISBN|978-1-509-52643-7}}
*{{Citation | last=Blackmore | first=Susan | year=2005| title=Consciousness: A Very Short Introduction | publisher=Oxford University Press |author-link=Susan Blackmore }}
* {{Citation | last=Bostrom | first=Nick | year=2014| title=Superintelligence: Paths, Dangers, Strategies | publisher=Oxford University Press |author-link=Nick Bostrom | title-link=Superintelligence: Paths, Dangers, Strategies }}, {{ISBN|978-0-19-967811-2}}
* {{Citation | last=Brooks | first=Rodney | author-link=Rodney Brooks | year =1990 | title = Elephants Don't Play Chess | journal = Robotics and Autonomous Systems | volume=6 | issue=1–2 | pages=3–15  | url=http://people.csail.mit.edu/brooks/papers/elephants.pdf | access-date=2007-08-30 | doi=10.1016/S0921-8890(05)80025-9| citeseerx=10.1.1.588.7539 }}
*[[Joanna Bryson|Bryson, Joanna]] (2019). The Artificial Intelligence of the Ethics of Artificial Intelligence: An Introductory Overview for Law and Regulation, 34.
* {{Citation | last=Chalmers | first=David J | author-link=David Chalmers | year=1996 | title=The Conscious Mind: In Search of a Fundamental Theory| publisher=Oxford University Press, New York | isbn=978-0-19-511789-9}}
* {{Citation | last=Cole | first=David | contribution=The Chinese Room Argument | title=The Stanford Encyclopedia of Philosophy | date = Fall 2004 | editor-first=Edward N. | editor-last = Zalta | url=http://plato.stanford.edu/archives/fall2004/entries/chinese-room/ }}.
*[[Kate Crawford|Crawford, Kate]] (2021). ''[[Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence]]''. Yale University Press.
* {{Crevier 1993}}
* {{Citation | last=Dennett | first=Daniel | author-link=Daniel Dennett | year=1991 | title=Consciousness Explained | publisher=The Penguin Press | isbn= 978-0-7139-9037-9| title-link=Consciousness Explained }}
* {{Citation | last=Dreyfus | first=Hubert | year =1972 | title = What Computers Can't Do  | publisher = MIT Press | location = New York | author-link = Hubert Dreyfus | isbn = 978-0-06-011082-6 | title-link=What Computers Can't Do }}
* {{Citation | last=Dreyfus | first=Hubert | year =1979 | title = What Computers ''Still'' Can't Do | publisher = MIT Press | location = New York | author-link = Hubert Dreyfus }}.
* {{Citation | last1=Dreyfus | first1=Hubert | last2 = Dreyfus | first2 = Stuart | year = 1986 | title = Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer | publisher = Blackwell | location = Oxford, UK | author-link = Hubert Dreyfus }}
* {{Citation | last=Fearn | first = Nicholas | year =2007 | title= The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World's Greatest Thinkers | publisher = Grove Press | location=New York }}
* {{Citation | last=Gladwell | first=Malcolm | title=Blink: The Power of Thinking Without Thinking| location=Boston | publisher=Little, Brown | year=2005 | isbn= 978-0-316-17232-5 |author-link= Malcolm Gladwell| title-link=Blink (book) }}.
* {{Citation | last=Harnad | first = Stevan | year=2001| contribution=What's Wrong and Right About Searle's Chinese Room Argument? | editor-first=M. | editor-last = Bishop | editor2-first = J. | editor2-last = Preston | title = Essays on Searle's Chinese Room Argument | publisher = Oxford University Press | url=http://cogprints.org/4023/1/searlbook.htm | author-link = Stevan Harnad }}
*[[Donna Haraway|Haraway, Donna]] (1985). ''[[A Cyborg Manifesto]]''.
* {{Citation | last = Haugeland | first = John|title = Artificial Intelligence: The Very Idea | publisher =  MIT Press | location = Cambridge, Mass. | year = 1985|author-link=John Haugeland}}.
* {{Citation | last=Hofstadter | first = Douglas | title = Gödel, Escher, Bach: an Eternal Golden Braid | year = 1979 | author-link = Douglas Hofstadter | title-link = Gödel, Escher, Bach }}.
* {{Citation | last=Horst | first= Steven | contribution =The Computational Theory of Mind | title= The Stanford Encyclopedia of Philosophy | date = 2009 | editor-first = Edward N. | editor-last = Zalta | url = http://plato.stanford.edu/entries/computational-mind/ | publisher= Metaphysics Research Lab, Stanford University }}.
* {{Citation | last1=Kaplan | first1=Andreas | author-link=Andreas Kaplan | last2=Haenlein | first2=Michael | year =2018 | title = Siri, Siri in my Hand, who's the Fairest in the Land? On the Interpretations, Illustrations and Implications of Artificial Intelligence | journal = Business Horizons | volume=62 | pages=15–25 |doi=10.1016/j.bushor.2018.08.004| s2cid=158433736 }}
* {{Citation | last=Kurzweil | first = Ray | title = The Singularity is Near | year = 2005 | publisher = Viking Press | location = New York | author-link = Ray Kurzweil | isbn=978-0-670-03384-3| title-link = The Singularity is Near }}.
* {{Citation | last=Lucas | first=John| year = 1961 | contribution=Minds, Machines and Gödel | editor-last =Anderson |editor-first =A.R. | title=Minds and Machines | url = http://users.ox.ac.uk/~jrlucas/Godel/mmg.html |author-link = John Lucas (philosopher)}}.
*[[Catherine Malabou|Malabou, Catherine]] (2019). ''Morphing Intelligence: From IQ Measurement to Artificial Brains''. (C. Shread, Trans.). Columbia University Press.
* {{Citation | last1 = McCarthy | first1 = John | last2 = Minsky | first2 = Marvin | last3 = Rochester | first3 = Nathan | last4 = Shannon | first4 = Claude | url = http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html | title = A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence | year = 1955 | author-link = John McCarthy (computer scientist) | author2-link = Marvin Minsky | author3-link = Nathan Rochester | author4-link = Claude Shannon | url-status = dead | archive-url = https://web.archive.org/web/20080930164306/http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html | archive-date = 2008-09-30 }}.
* {{citation
| last = McCarthy
| first = John
| author-link = John McCarthy (computer scientist)
| year = 1999
| title = What is AI?
| url = http://jmc.stanford.edu/artificial-intelligence/what-is-ai/index.html
| access-date = 4 December 2022
| archive-date = 4 December 2022
| archive-url = https://web.archive.org/web/20221204051737/http://jmc.stanford.edu/artificial-intelligence/what-is-ai/index.html
| url-status = live
}}
* {{Citation | last=McDermott | first=Drew | title=How Intelligent is Deep Blue | url=http://www.psych.utoronto.ca/~reingold/courses/ai/cache/mcdermott.html | newspaper=New York Times | date=May 14, 1997 | access-date=October 10, 2007 | archive-url=https://web.archive.org/web/20071004234354/http://www.psych.utoronto.ca/~reingold/courses/ai/cache/mcdermott.html | archive-date=October 4, 2007 | url-status=dead }}
* {{Citation | last=Moravec | first=Hans | year = 1988 | title = Mind Children | publisher = Harvard University Press | author-link =Hans Moravec }}
* {{Citation | last=Penrose |first=Roger|title= The Emperor's New Mind: Concerning Computers, Minds, and The Laws of Physics|publisher= Oxford University Press| year=1989|isbn=978-0-14-014534-2 |author-link=Roger Penrose|title-link=The Emperor's New Mind}}c
* [https://plato.stanford.edu/archives/fall2020/entries/computational-mind/ Rescorla, Michael, "The Computational Theory of Mind", in:Edward N. Zalta (ed.), ''The Stanford Encyclopedia of Philosophy'' (Fall 2020 Edition)]
* {{Russell Norvig 2003}}
* {{Citation | last=Searle | first=John | year=1980 | url=http://www.class.uh.edu/phil/garson/MindsBrainsandPrograms.pdf | title=Minds, Brains and Programs | journal=Behavioral and Brain Sciences | volume=3 | issue=3 | pages=417–457 | author-link=John Searle | doi=10.1017/S0140525X00005756 | s2cid=55303721 | url-status=dead | archive-url=https://web.archive.org/web/20150923204614/http://www.class.uh.edu/phil/garson/MindsBrainsandPrograms.pdf | archive-date=2015-09-23 }}
* {{Citation | last=Searle | first=John | author-link=John Searle | year=1992 | title=The Rediscovery of the Mind|publisher=M.I.T. Press|location=Cambridge, Massachusetts}}
* {{Citation | last = Searle | first = John | author-link = John Searle | year = 1999 | title = Mind, language and society | isbn = 978-0-465-04521-1 | publisher = Basic Books | location = New York, NY | oclc = 231867665 | url = https://archive.org/details/mindlanguagesoci00sear }}
* {{Turing 1950}}

{{Philosophy of mind}}
{{Philosophy of science}}
{{Psychology}}
{{Philosophy topics}}
{{Authority control}}

{{DEFAULTSORT:Philosophy Of Artificial Intelligence}}
[[Category:Philosophy of artificial intelligence| ]]
[[Category:Philosophy of science]]
[[Category:Philosophy of technology]]
[[Category:Philosophy of mind|Artificial]]
[[Category:Open problems]]
[[Category:Articles containing video clips]]